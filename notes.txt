Using boto3 to interact with AWS services. 

Put the cors for the lambda functions as the sites url and not allow anyone to touch the lambda. 

OAC and signedurls can't be set at the same time

APIGatway should specify at the exact path source_arn 

HMAC or Cryptography modules doesn't work with cloudfront for signing urls
only RSA works 

When JavaScript running on your website (e.g., from index.js) tries to make the same request, the browser does enforce CORS, and the API must allow the origin of your website to access it.

Your website is hosted at https://mywebsite.com.
Your API is hosted at https://api.example.com.
JavaScript tries to fetch data from the API, but the API doesn't explicitly allow https://mywebsite.com in its CORS configuration. The browser blocks the request.

Direct access to the API URL in the browser bypasses CORS, so it doesn't indicate whether JavaScript will encounter a CORS issue.
JavaScript requests enforce CORS. Check the browser console and network tab to identify CORS issues.

Expected Behavior
Preflight OPTIONS Requests:

When the browser sends a preflight request to check if the API allows the request, it will invoke the OPTIONS method.
API Gateway will:
Use the MOCK integration to immediately return a 200 OK response.
Include CORS headers (Access-Control-Allow-Origin, Access-Control-Allow-Methods, Access-Control-Allow-Headers) in the response.
Example Response for OPTIONS Request:
    
    HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET,OPTIONS
Access-Control-Allow-Headers: Content-Type,Authorization

Having response_parameters = { ... } with all true values is not enough by itself because true does not define actual values for the CORS headers. 
It only enables the headers to be passed from the backend or integration response to the client. 
If the backend or integration does not provide those headers, the response will not include the CORS headers, and the browser will block the request.

The rest is in CORS setup in chatgpt

CORS, IAM, source ARN in API Gateway made the biggest nightmare 

curl -X POST \
  -H "Content-Type: application/json" \
  -d '{"email": "sam.albershtein@gmail.com"}' \
  <YOUR_API_ENDPOINT>

.gitignore only applies to files and folders not yet tracked by Git. That's why untracking the folder with git rm --cached is necessary.
Adding .terraform/ ensures all .terraform folders in your repository are ignored. If you only want to ignore a specific .terraform folder, use its relative path
git rm -r --cached .terraform 
Simply adding the folder to .gitignore does not remove it from the repository. You need to explicitly untrack it.

public-read in git workflows made a lot of hassle:
|
v

        '''
        public-read will not work If the bucket has the Block Public ACLs option enabled, any attempt to apply a public ACL (like --acl public-read) will be rejected, even if you try to explicitly set it during upload.
        This is a security feature to prevent accidental public exposure of sensitive data.

        resource "aws_s3_bucket_public_access_block" "example" {
          bucket                  = aws_s3_bucket.example.id
          block_public_acls       = true
          ignore_public_acls      = true
          block_public_policy     = true
          restrict_public_buckets = true
        }

          With block_public_acls = true, the bucket ignores or outright rejects any public ACLs, like public-read.

          Your bucket is configured with BucketOwnerPreferred for object ownership. This ensures that the bucket owner takes ownership of any objects uploaded to the bucket, even if they come with a public ACL.
          However, this does not override "Block Public ACLs" settings. The public ACL is still stripped or rejected.

          If the aws_s3_bucket_public_access_block resource was not explicitly defined, AWS applies the following defaults for new S3 buckets:

          *Block public access for ACLs and policies enabled by default:
          This happens automatically unless you disable the "Block Public Access" feature at the bucket level or account level.
          The AWS Management Console applies these settings when creating new buckets, even if the aws_s3_bucket_public_access_block resource is not explicitly set in Terraform.

          You can omit the aws_s3_bucket_public_access_block resource if:

          You do not need additional restrictions on public ACLs or policies, and
          You are relying on the default AWS settings to block public access, or
          You explicitly handle access via ACLs, ownership controls, and/or bucket policies.

          Defining the public access block in Terraform makes your intention explicit. 
          Anyone reading the Terraform code will immediately understand that public access is blocked. It avoids relying on implicit defaults, which may not always be clear to other developers or future maintainers.

          You can safely omit aws_s3_bucket_public_access_block if:

          The bucket is always private by design:

          Your workflow ensures that no public ACLs or bucket policies are used.
          All access is managed through explicit bucket policies and ACLs.
          You're confident in AWS defaults:

          If you trust that AWS will continue to apply its default behavior of blocking public access for new buckets.

          The main point of explicitly setting aws_s3_bucket_public_access_block is to:

          Make your intentions explicit and clear in code.
          Avoid reliance on mutable AWS defaults.
          Ensure consistency, auditing, and compliance.
          If these points are not critical for your use case and you trust AWS defaults, you can omit the resource. However, for production setups or collaborative environments, explicit configuration is a best practice.

        '''

        Add zip into terraform

        Backend Ci/CD awaits for variables: 

        We declared them in github secrets and need to assign TF_VAR_ in the back-end.cicd.yml 

        Your variables.tf file should define the required variables cloudfront_private_key and cloudfront_key_pair_id in a structured way, ensuring Terraform knows their expected type and purpose.

        We will not do this here but good practice is to create a separate S3 bucket for the terraform.tfstate 

        and a dynamodb for the lock of the tfstate 

terraform {
  backend "s3" {
    bucket         = "your-s3-bucket-name"
    key            = "path/to/terraform.tfstate"
    region         = "your-region"
    dynamodb_table = "terraform-lock-table"
  }
}

provider "aws" {
  region = "your-region"
}

resource "aws_s3_bucket" "terraform_state" {
  bucket = "your-s3-bucket-name"

  versioning {
    enabled = true
  }

  lifecycle {
    prevent_destroy = true  # Prevent accidental deletion
  }
}

resource "aws_dynamodb_table" "terraform_lock" {
  name         = "terraform-lock-table"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}

Terraform knows to update the tfstate file in the S3 
bucket because the backend configuration in main.tf tells it where the state file should be stored.

When you configure the backend "s3" in your main.tf and run terraform init, 
Terraform connects to the specified S3 bucket and DynamoDB table (if used).

Terraform stores and retrieves the terraform.tfstate file from the key path in the specified S3 bucket.

All changes to infrastructure (terraform apply) will update this remote state file.

If a DynamoDB table is configured for locking (dynamodb_table = "terraform-lock-table"), 
Terraform locks the state file during operations to prevent concurrent modifications.

How Other Users or Systems Know to Communicate with the State File
Other users or CI/CD systems need the same backend configuration to access the remote state. Here's how to set it up:

1. Ensure the Backend Configuration is Shared
The backend "s3" block in main.tf must be shared across all users or 
systems working with the same Terraform configuration. Example:

Users and CI/CD systems must have AWS credentials to access the S3 bucket and DynamoDB table.

IAM Roles:

For systems running in AWS (e.g., EC2, Lambda), 
use an IAM role with the necessary permissions attached.

For S3:

json
Copy code
{
  "Effect": "Allow",
  "Action": [
    "s3:GetObject",
    "s3:PutObject",
    "s3:ListBucket",
    "s3:DeleteObject"
  ],
  "Resource": [
    "arn:aws:s3:::your-s3-bucket-name",
    "arn:aws:s3:::your-s3-bucket-name/*"
  ]
}

For DynamoDB (if using locking):

json
Copy code
{
  "Effect": "Allow",
  "Action": [
    "dynamodb:PutItem",
    "dynamodb:GetItem",
    "dynamodb:DeleteItem",
    "dynamodb:DescribeTable"
  ],
  "Resource": "arn:aws:dynamodb:your-region:your-account-id:table/terraform-lock-table"
}

Run terraform init:

When another user or system runs terraform init, 
Terraform downloads the current remote state from the S3 bucket. You’ll see output like

After a successful operation (terraform apply), check the S3 bucket to verify the terraform.tfstate file is updated.

If DynamoDB locking is enabled, check the DynamoDB table for a lock entry during terraform plan or terraform apply.

How the dynamodb locks the object in the bucket?

DynamoDB does not directly lock objects in the S3 bucket, but it works with Terraform to implement 
a state locking mechanism to prevent simultaneous operations on the same Terraform state file.

DynamoDB does not directly lock or interact with the S3 bucket or its objects.
Instead:
Terraform writes to the S3 bucket for state file management (e.g., storing terraform.tfstate).
Terraform uses DynamoDB to coordinate access to the state file, ensuring that only one process modifies the state at a time.

If locking is not used (e.g., you don’t configure a DynamoDB table):

Multiple users or processes can modify the state file simultaneously, leading to:
State Corruption: Conflicting updates can result in an inconsistent or invalid state.
Infrastructure Drift: Terraform might apply incorrect changes due to outdated or incomplete state information.

Terraform Plan/Apply Starts:

Terraform writes a lock record to DynamoDB.
Other Users/Processes Wait:

If another plan or apply is attempted, Terraform checks the DynamoDB table for an existing lock:
If a lock exists, the new process waits or fails, depending on the configuration.
Terraform Plan/Apply Completes:

Terraform deletes the lock record from DynamoDB.


resource "aws_route53_record" "example" is bad because we have are domain in cloudflare and not in route53 



terraform console
aws_acm_certificate.cert_for_cloudflare_dns.domain_validation_options
