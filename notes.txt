Using boto3 to interact with AWS services. 

Put the cors for the lambda functions as the sites url and not allow anyone to touch the lambda. 

OAC and signedurls can't be set at the same time

APIGatway should specify at the exact path source_arn 

HMAC or Cryptography modules doesn't work with cloudfront for signing urls
only RSA works 

When JavaScript running on your website (e.g., from index.js) tries to make the same request, the browser does enforce CORS, and the API must allow the origin of your website to access it.

Your website is hosted at https://mywebsite.com.
Your API is hosted at https://api.example.com.
JavaScript tries to fetch data from the API, but the API doesn't explicitly allow https://mywebsite.com in its CORS configuration. The browser blocks the request.

Direct access to the API URL in the browser bypasses CORS, so it doesn't indicate whether JavaScript will encounter a CORS issue.
JavaScript requests enforce CORS. Check the browser console and network tab to identify CORS issues.

Expected Behavior
Preflight OPTIONS Requests:

When the browser sends a preflight request to check if the API allows the request, it will invoke the OPTIONS method.
API Gateway will:
Use the MOCK integration to immediately return a 200 OK response.
Include CORS headers (Access-Control-Allow-Origin, Access-Control-Allow-Methods, Access-Control-Allow-Headers) in the response.
Example Response for OPTIONS Request:
    
    HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET,OPTIONS
Access-Control-Allow-Headers: Content-Type,Authorization

Having response_parameters = { ... } with all true values is not enough by itself because true does not define actual values for the CORS headers. 
It only enables the headers to be passed from the backend or integration response to the client. 
If the backend or integration does not provide those headers, the response will not include the CORS headers, and the browser will block the request.

The rest is in CORS setup in chatgpt

CORS, IAM, source ARN in API Gateway made the biggest nightmare 

curl -X POST \
  -H "Content-Type: application/json" \
  -d '{"email": "sam.albershtein@gmail.com"}' \
  <YOUR_API_ENDPOINT>

.gitignore only applies to files and folders not yet tracked by Git. That's why untracking the folder with git rm --cached is necessary.
Adding .terraform/ ensures all .terraform folders in your repository are ignored. If you only want to ignore a specific .terraform folder, use its relative path
git rm -r --cached .terraform 
Simply adding the folder to .gitignore does not remove it from the repository. You need to explicitly untrack it.

public-read in git workflows made a lot of hassle:
|
v

        '''
        public-read will not work If the bucket has the Block Public ACLs option enabled, any attempt to apply a public ACL (like --acl public-read) will be rejected, even if you try to explicitly set it during upload.
        This is a security feature to prevent accidental public exposure of sensitive data.

        resource "aws_s3_bucket_public_access_block" "example" {
          bucket                  = aws_s3_bucket.example.id
          block_public_acls       = true
          ignore_public_acls      = true
          block_public_policy     = true
          restrict_public_buckets = true
        }

          With block_public_acls = true, the bucket ignores or outright rejects any public ACLs, like public-read.

          Your bucket is configured with BucketOwnerPreferred for object ownership. This ensures that the bucket owner takes ownership of any objects uploaded to the bucket, even if they come with a public ACL.
          However, this does not override "Block Public ACLs" settings. The public ACL is still stripped or rejected.

          If the aws_s3_bucket_public_access_block resource was not explicitly defined, AWS applies the following defaults for new S3 buckets:

          *Block public access for ACLs and policies enabled by default:
          This happens automatically unless you disable the "Block Public Access" feature at the bucket level or account level.
          The AWS Management Console applies these settings when creating new buckets, even if the aws_s3_bucket_public_access_block resource is not explicitly set in Terraform.

          You can omit the aws_s3_bucket_public_access_block resource if:

          You do not need additional restrictions on public ACLs or policies, and
          You are relying on the default AWS settings to block public access, or
          You explicitly handle access via ACLs, ownership controls, and/or bucket policies.

          Defining the public access block in Terraform makes your intention explicit. 
          Anyone reading the Terraform code will immediately understand that public access is blocked. It avoids relying on implicit defaults, which may not always be clear to other developers or future maintainers.

          You can safely omit aws_s3_bucket_public_access_block if:

          The bucket is always private by design:

          Your workflow ensures that no public ACLs or bucket policies are used.
          All access is managed through explicit bucket policies and ACLs.
          You're confident in AWS defaults:

          If you trust that AWS will continue to apply its default behavior of blocking public access for new buckets.

          The main point of explicitly setting aws_s3_bucket_public_access_block is to:

          Make your intentions explicit and clear in code.
          Avoid reliance on mutable AWS defaults.
          Ensure consistency, auditing, and compliance.
          If these points are not critical for your use case and you trust AWS defaults, you can omit the resource. However, for production setups or collaborative environments, explicit configuration is a best practice.

        '''

        Add zip into terraform

        Backend Ci/CD awaits for variables: 

        We declared them in github secrets and need to assign TF_VAR_ in the back-end.cicd.yml 

        Your variables.tf file should define the required variables cloudfront_private_key and cloudfront_key_pair_id in a structured way, ensuring Terraform knows their expected type and purpose.

        We will not do this here but good practice is to create a separate S3 bucket for the terraform.tfstate 

        and a dynamodb for the lock of the tfstate 

terraform {
  backend "s3" {
    bucket         = "your-s3-bucket-name"
    key            = "path/to/terraform.tfstate"
    region         = "your-region"
    dynamodb_table = "terraform-lock-table"
  }
}

provider "aws" {
  region = "your-region"
}

resource "aws_s3_bucket" "terraform_state" {
  bucket = "your-s3-bucket-name"

  versioning {
    enabled = true
  }

  lifecycle {
    prevent_destroy = true  # Prevent accidental deletion
  }
}

resource "aws_dynamodb_table" "terraform_lock" {
  name         = "terraform-lock-table"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}

Terraform knows to update the tfstate file in the S3 
bucket because the backend configuration in main.tf tells it where the state file should be stored.

When you configure the backend "s3" in your main.tf and run terraform init, 
Terraform connects to the specified S3 bucket and DynamoDB table (if used).

Terraform stores and retrieves the terraform.tfstate file from the key path in the specified S3 bucket.

All changes to infrastructure (terraform apply) will update this remote state file.

If a DynamoDB table is configured for locking (dynamodb_table = "terraform-lock-table"), 
Terraform locks the state file during operations to prevent concurrent modifications.

How Other Users or Systems Know to Communicate with the State File
Other users or CI/CD systems need the same backend configuration to access the remote state. Here's how to set it up:

1. Ensure the Backend Configuration is Shared
The backend "s3" block in main.tf must be shared across all users or 
systems working with the same Terraform configuration. Example:

Users and CI/CD systems must have AWS credentials to access the S3 bucket and DynamoDB table.

IAM Roles:

For systems running in AWS (e.g., EC2, Lambda), 
use an IAM role with the necessary permissions attached.

For S3:

json
Copy code
{
  "Effect": "Allow",
  "Action": [
    "s3:GetObject",
    "s3:PutObject",
    "s3:ListBucket",
    "s3:DeleteObject"
  ],
  "Resource": [
    "arn:aws:s3:::your-s3-bucket-name",
    "arn:aws:s3:::your-s3-bucket-name/*"
  ]
}

For DynamoDB (if using locking):

json
Copy code
{
  "Effect": "Allow",
  "Action": [
    "dynamodb:PutItem",
    "dynamodb:GetItem",
    "dynamodb:DeleteItem",
    "dynamodb:DescribeTable"
  ],
  "Resource": "arn:aws:dynamodb:your-region:your-account-id:table/terraform-lock-table"
}

Run terraform init:

When another user or system runs terraform init, 
Terraform downloads the current remote state from the S3 bucket. You’ll see output like

After a successful operation (terraform apply), check the S3 bucket to verify the terraform.tfstate file is updated.

If DynamoDB locking is enabled, check the DynamoDB table for a lock entry during terraform plan or terraform apply.

How the dynamodb locks the object in the bucket?

DynamoDB does not directly lock objects in the S3 bucket, but it works with Terraform to implement 
a state locking mechanism to prevent simultaneous operations on the same Terraform state file.

DynamoDB does not directly lock or interact with the S3 bucket or its objects.
Instead:
Terraform writes to the S3 bucket for state file management (e.g., storing terraform.tfstate).
Terraform uses DynamoDB to coordinate access to the state file, ensuring that only one process modifies the state at a time.

If locking is not used (e.g., you don’t configure a DynamoDB table):

Multiple users or processes can modify the state file simultaneously, leading to:
State Corruption: Conflicting updates can result in an inconsistent or invalid state.
Infrastructure Drift: Terraform might apply incorrect changes due to outdated or incomplete state information.

Terraform Plan/Apply Starts:

Terraform writes a lock record to DynamoDB.
Other Users/Processes Wait:

If another plan or apply is attempted, Terraform checks the DynamoDB table for an existing lock:
If a lock exists, the new process waits or fails, depending on the configuration.
Terraform Plan/Apply Completes:

Terraform deletes the lock record from DynamoDB.


resource "aws_route53_record" "example" is bad because we have are domain in cloudflare and not in route53 



terraform console
aws_acm_certificate.cert_for_cloudflare_dns.domain_validation_options

1. Resource-Based Policies (e.g., Bucket Policies)
Standalone Policies: These policies are directly attached to a resource (e.g., an S3 bucket, an SNS topic, or an SQS queue). They define who (users, roles, accounts) can access that specific resource.
Use Case:
Cross-account access (e.g., another AWS account accessing your S3 bucket).
Public access (e.g., making an S3 bucket publicly readable).
Examples:
S3 Bucket Policy
SNS Topic Policy
SQS Queue Policy

Key Notes for Resource-Based Policies:
Directly attached to a specific resource.
No need for a role unless you’re combining it with another service.

2. Identity-Based Policies
These are attached to IAM entities (users, groups, or roles) to grant them permissions to access AWS resources.

Use Case:
Allow a user, group, or role to access resources like S3, EC2, DynamoDB, etc.
Usually combined with an IAM Role when a service needs to assume permissions.
Examples:
Attached to a User:
Grant a user access to an S3 bucket.
Attached to a Group:
Grant a group access to perform administrative tasks.
Attached to a Role:
Allow a Lambda function to read/write from DynamoDB.

3. When Policies Need to Be Part of a Role
When services (e.g., Lambda, EC2, ECS) need access to AWS resources, the policy must be attached to a role because these services assume roles to obtain permissions.

Why?
AWS services like Lambda and EC2 cannot directly use IAM policies. Instead, they assume an IAM role that has the required permissions.
This provides temporary credentials to the service for secure operations.

Type of Policy	               Attached To                                 	Use Case
Resource-Based Policy	S3 Bucket, SNS, SQS, etc.	Directly grant access to specific resources (e.g., make a bucket public).

Identity-Based Policy	IAM Users, Groups, or Roles	Grant permissions to identities to access various AWS services/resources.

IAM Role with Policies	Services (e.g., Lambda, EC2)	Provide temporary credentials for AWS services to interact with resources.

Resource-Based Policies: Standalone policies directly attached to resources.
Identity-Based Policies: Require users, groups, or roles for attachment.
IAM Roles: Required for AWS services to assume permissions using policies.

what is  Action    = "sts:AssumeRole"  ?

The sts:AssumeRole action is part of AWS Security Token Service (STS) and is used to allow a trusted entity 
(e.g., a user, application, or service) to assume an IAM role. When an entity assumes a role, it temporarily gains the permissions defined by that role's attached policies.


Key Concepts
AWS Security Token Service (STS):

AWS STS provides temporary security credentials for IAM roles. These credentials are used to access AWS resources.
Assuming a Role:

A user, service, or another AWS account can assume a role to temporarily acquire its permissions.
The sts:AssumeRole action is required to allow an entity to assume the role.


Use Case for sts:AssumeRole
The sts:AssumeRole action is specified in the trust policy of an IAM role. It defines who can assume the role.

Example: Allowing EC2 to Assume a Role
You want an EC2 instance to access an S3 bucket. To do this, you:

Create an IAM role with s3 permissions.
Allow the EC2 service (ec2.amazonaws.com) to assume this role using sts:AssumeRole.
Trust Policy Example: This policy allows the EC2 service to assume the role:

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}

Structure of a Trust Policy
A trust policy uses sts:AssumeRole to define:

Effect: Allow or deny the assume role action.
Principal: Who can assume the role (e.g., services like EC2 or specific AWS accounts).
Action: The sts:AssumeRole action.

How sts:AssumeRole Works
Trust Policy:

Defines who (a user, service, or account) is allowed to assume the role.
AWS STS Call:

The trusted entity (e.g., EC2) makes an AssumeRole API call.
AWS STS generates temporary credentials for the role.
Temporary Credentials:

The temporary credentials include an access key, secret key, and session token.
These credentials allow the entity to access resources based on the permissions attached to the role.

Use Cases for sts:AssumeRole
Cross-Account Access:

Allow a user or role in Account B to access resources in Account A.
AWS Service Access:

Allow services like EC2, Lambda, or ECS tasks to assume roles for permissions.
Federated Access:

Allow external identity providers (e.g., Active Directory, Okta) to assume roles for accessing AWS resources.
Temporary Access:

Grant temporary access to a resource without needing to manage long-term credentials.


How It Looks in Terraform
In Terraform, the sts:AssumeRole action is typically used in the trust policy of a role.

Example:
Allow Lambda to assume a role:

resource "aws_iam_role" "lambda_role" {
  name = "lambda-access-s3-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect    = "Allow"
        Principal = { Service = "lambda.amazonaws.com" }
        Action    = "sts:AssumeRole"
      }
    ]
  })
}

Attach a policy to grant S3 access:

resource "aws_iam_role_policy" "lambda_policy" {
  name   = "lambda-s3-policy"
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect   = "Allow"
        Action   = ["s3:PutObject", "s3:GetObject"]
        Resource = "arn:aws:s3:::my-bucket/*"
      }
    ]
  })
}

sts:AssumeRole is an AWS STS action that allows a trusted entity (user, service, account) to temporarily assume an IAM role.
It’s defined in the trust policy of an IAM role.
Used for enabling cross-account access, AWS service permissions, and temporary credentials.

When creating a bucket for Terraform state storage (.tfstate), it should be private for security reasons. Terraform state files can contain sensitive information such as:

Access keys
Resource configurations
Secrets (e.g., database passwords, API keys)

Why the Bucket Should Be Private
Security: A public bucket exposes your infrastructure's sensitive data to anyone with access to the bucket URL.
Controlled Access: Only authorized users or roles should have access to the bucket.
No Need for CloudFront: CloudFront is not required for accessing the .tfstate bucket because:
Terraform clients directly access the bucket.
There’s no need for caching or CDN benefits for .tfstate files.


            'headers': {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET,OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type,Authorization'
            },

            mandatory :P 



# SUUUUUUUUPER IMPORTANT TO ADD ALL DEPENDS ON INTEGRATIONS!!!!!!!!! (WE HAD BIG PROBLEMS WITH THE API GATEWAY NOT BEING ABLE TO CONTACT THE CRYPTO LAMBDA)
# API Deployment (remove 'stage_name' attribute)
resource "aws_api_gateway_deployment" "api_deployment" {
  depends_on = [
    aws_api_gateway_integration.lambda_integration,
    aws_api_gateway_integration.send_cv_integration,
    aws_api_gateway_method.viewer_count_options,
    aws_api_gateway_method.send_cv_options,
    aws_api_gateway_integration.crypto_api_integration 
  ]
  rest_api_id = aws_api_gateway_rest_api.viewer_count_api.id
}

ALSO THE CODE worked within lambda but not if we tested inapi gateway! NEEDED A CODE REPAIR TOO! 

Request module was required for import requests in ctypto lambda