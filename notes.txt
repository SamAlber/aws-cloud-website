Using boto3 to interact with AWS services. 

Put the cors for the lambda functions as the sites url and not allow anyone to touch the lambda. 

OAC and signedurls can't be set at the same time

APIGatway should specify at the exact path source_arn 

HMAC or Cryptography modules doesn't work with cloudfront for signing urls
only RSA works 

When JavaScript running on your website (e.g., from index.js) tries to make the same request, the browser does enforce CORS, and the API must allow the origin of your website to access it.

Your website is hosted at https://mywebsite.com.
Your API is hosted at https://api.example.com.
JavaScript tries to fetch data from the API, but the API doesn't explicitly allow https://mywebsite.com in its CORS configuration. The browser blocks the request.

Direct access to the API URL in the browser bypasses CORS, so it doesn't indicate whether JavaScript will encounter a CORS issue.
JavaScript requests enforce CORS. Check the browser console and network tab to identify CORS issues.

Expected Behavior
Preflight OPTIONS Requests:

When the browser sends a preflight request to check if the API allows the request, it will invoke the OPTIONS method.
API Gateway will:
Use the MOCK integration to immediately return a 200 OK response.
Include CORS headers (Access-Control-Allow-Origin, Access-Control-Allow-Methods, Access-Control-Allow-Headers) in the response.
Example Response for OPTIONS Request:
    
    HTTP/1.1 200 OK
Access-Control-Allow-Origin: *
Access-Control-Allow-Methods: GET,OPTIONS
Access-Control-Allow-Headers: Content-Type,Authorization

Having response_parameters = { ... } with all true values is not enough by itself because true does not define actual values for the CORS headers. 
It only enables the headers to be passed from the backend or integration response to the client. 
If the backend or integration does not provide those headers, the response will not include the CORS headers, and the browser will block the request.

The rest is in CORS setup in chatgpt

CORS, IAM, source ARN in API Gateway made the biggest nightmare 

curl -X POST \
  -H "Content-Type: application/json" \
  -d '{"email": "sam.albershtein@gmail.com"}' \
  <YOUR_API_ENDPOINT>

.gitignore only applies to files and folders not yet tracked by Git. That's why untracking the folder with git rm --cached is necessary.
Adding .terraform/ ensures all .terraform folders in your repository are ignored. If you only want to ignore a specific .terraform folder, use its relative path
git rm -r --cached .terraform 
Simply adding the folder to .gitignore does not remove it from the repository. You need to explicitly untrack it.

public-read in git workflows made a lot of hassle:
|
v

        '''
        public-read will not work If the bucket has the Block Public ACLs option enabled, any attempt to apply a public ACL (like --acl public-read) will be rejected, even if you try to explicitly set it during upload.
        This is a security feature to prevent accidental public exposure of sensitive data.

        resource "aws_s3_bucket_public_access_block" "example" {
          bucket                  = aws_s3_bucket.example.id
          block_public_acls       = true
          ignore_public_acls      = true
          block_public_policy     = true
          restrict_public_buckets = true
        }

          With block_public_acls = true, the bucket ignores or outright rejects any public ACLs, like public-read.

          Your bucket is configured with BucketOwnerPreferred for object ownership. This ensures that the bucket owner takes ownership of any objects uploaded to the bucket, even if they come with a public ACL.
          However, this does not override "Block Public ACLs" settings. The public ACL is still stripped or rejected.

          If the aws_s3_bucket_public_access_block resource was not explicitly defined, AWS applies the following defaults for new S3 buckets:

          *Block public access for ACLs and policies enabled by default:
          This happens automatically unless you disable the "Block Public Access" feature at the bucket level or account level.
          The AWS Management Console applies these settings when creating new buckets, even if the aws_s3_bucket_public_access_block resource is not explicitly set in Terraform.

          You can omit the aws_s3_bucket_public_access_block resource if:

          You do not need additional restrictions on public ACLs or policies, and
          You are relying on the default AWS settings to block public access, or
          You explicitly handle access via ACLs, ownership controls, and/or bucket policies.

          Defining the public access block in Terraform makes your intention explicit. 
          Anyone reading the Terraform code will immediately understand that public access is blocked. It avoids relying on implicit defaults, which may not always be clear to other developers or future maintainers.

          You can safely omit aws_s3_bucket_public_access_block if:

          The bucket is always private by design:

          Your workflow ensures that no public ACLs or bucket policies are used.
          All access is managed through explicit bucket policies and ACLs.
          You're confident in AWS defaults:

          If you trust that AWS will continue to apply its default behavior of blocking public access for new buckets.

          The main point of explicitly setting aws_s3_bucket_public_access_block is to:

          Make your intentions explicit and clear in code.
          Avoid reliance on mutable AWS defaults.
          Ensure consistency, auditing, and compliance.
          If these points are not critical for your use case and you trust AWS defaults, you can omit the resource. However, for production setups or collaborative environments, explicit configuration is a best practice.

        '''

        Add zip into terraform

        Backend Ci/CD awaits for variables: 

        We declared them in github secrets and need to assign TF_VAR_ in the back-end.cicd.yml 

        Your variables.tf file should define the required variables cloudfront_private_key and cloudfront_key_pair_id in a structured way, ensuring Terraform knows their expected type and purpose.

        We will not do this here but good practice is to create a separate S3 bucket for the terraform.tfstate 

        and a dynamodb for the lock of the tfstate 

terraform {
  backend "s3" {
    bucket         = "your-s3-bucket-name"
    key            = "path/to/terraform.tfstate"
    region         = "your-region"
    dynamodb_table = "terraform-lock-table"
  }
}

provider "aws" {
  region = "your-region"
}

resource "aws_s3_bucket" "terraform_state" {
  bucket = "your-s3-bucket-name"

  versioning {
    enabled = true
  }

  lifecycle {
    prevent_destroy = true  # Prevent accidental deletion
  }
}

resource "aws_dynamodb_table" "terraform_lock" {
  name         = "terraform-lock-table"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }
}

Terraform knows to update the tfstate file in the S3 
bucket because the backend configuration in main.tf tells it where the state file should be stored.

When you configure the backend "s3" in your main.tf and run terraform init, 
Terraform connects to the specified S3 bucket and DynamoDB table (if used).

Terraform stores and retrieves the terraform.tfstate file from the key path in the specified S3 bucket.

All changes to infrastructure (terraform apply) will update this remote state file.

If a DynamoDB table is configured for locking (dynamodb_table = "terraform-lock-table"), 
Terraform locks the state file during operations to prevent concurrent modifications.

How Other Users or Systems Know to Communicate with the State File
Other users or CI/CD systems need the same backend configuration to access the remote state. Here's how to set it up:

1. Ensure the Backend Configuration is Shared
The backend "s3" block in main.tf must be shared across all users or 
systems working with the same Terraform configuration. Example:

Users and CI/CD systems must have AWS credentials to access the S3 bucket and DynamoDB table.

IAM Roles:

For systems running in AWS (e.g., EC2, Lambda), 
use an IAM role with the necessary permissions attached.

For S3:

json
Copy code
{
  "Effect": "Allow",
  "Action": [
    "s3:GetObject",
    "s3:PutObject",
    "s3:ListBucket",
    "s3:DeleteObject"
  ],
  "Resource": [
    "arn:aws:s3:::your-s3-bucket-name",
    "arn:aws:s3:::your-s3-bucket-name/*"
  ]
}

For DynamoDB (if using locking):

json
Copy code
{
  "Effect": "Allow",
  "Action": [
    "dynamodb:PutItem",
    "dynamodb:GetItem",
    "dynamodb:DeleteItem",
    "dynamodb:DescribeTable"
  ],
  "Resource": "arn:aws:dynamodb:your-region:your-account-id:table/terraform-lock-table"
}

Run terraform init:

When another user or system runs terraform init, 
Terraform downloads the current remote state from the S3 bucket. You’ll see output like

After a successful operation (terraform apply), check the S3 bucket to verify the terraform.tfstate file is updated.

If DynamoDB locking is enabled, check the DynamoDB table for a lock entry during terraform plan or terraform apply.

How the dynamodb locks the object in the bucket?

DynamoDB does not directly lock objects in the S3 bucket, but it works with Terraform to implement 
a state locking mechanism to prevent simultaneous operations on the same Terraform state file.

DynamoDB does not directly lock or interact with the S3 bucket or its objects.
Instead:
Terraform writes to the S3 bucket for state file management (e.g., storing terraform.tfstate).
Terraform uses DynamoDB to coordinate access to the state file, ensuring that only one process modifies the state at a time.

If locking is not used (e.g., you don’t configure a DynamoDB table):

Multiple users or processes can modify the state file simultaneously, leading to:
State Corruption: Conflicting updates can result in an inconsistent or invalid state.
Infrastructure Drift: Terraform might apply incorrect changes due to outdated or incomplete state information.

Terraform Plan/Apply Starts:

Terraform writes a lock record to DynamoDB.
Other Users/Processes Wait:

If another plan or apply is attempted, Terraform checks the DynamoDB table for an existing lock:
If a lock exists, the new process waits or fails, depending on the configuration.
Terraform Plan/Apply Completes:

Terraform deletes the lock record from DynamoDB.


resource "aws_route53_record" "example" is bad because we have are domain in cloudflare and not in route53 



terraform console
aws_acm_certificate.cert_for_cloudflare_dns.domain_validation_options

1. Resource-Based Policies (e.g., Bucket Policies)
Standalone Policies: These policies are directly attached to a resource (e.g., an S3 bucket, an SNS topic, or an SQS queue). They define who (users, roles, accounts) can access that specific resource.
Use Case:
Cross-account access (e.g., another AWS account accessing your S3 bucket).
Public access (e.g., making an S3 bucket publicly readable).
Examples:
S3 Bucket Policy
SNS Topic Policy
SQS Queue Policy

Key Notes for Resource-Based Policies:
Directly attached to a specific resource.
No need for a role unless you’re combining it with another service.

2. Identity-Based Policies
These are attached to IAM entities (users, groups, or roles) to grant them permissions to access AWS resources.

Use Case:
Allow a user, group, or role to access resources like S3, EC2, DynamoDB, etc.
Usually combined with an IAM Role when a service needs to assume permissions.
Examples:
Attached to a User:
Grant a user access to an S3 bucket.
Attached to a Group:
Grant a group access to perform administrative tasks.
Attached to a Role:
Allow a Lambda function to read/write from DynamoDB.

3. When Policies Need to Be Part of a Role
When services (e.g., Lambda, EC2, ECS) need access to AWS resources, the policy must be attached to a role because these services assume roles to obtain permissions.

Why?
AWS services like Lambda and EC2 cannot directly use IAM policies. Instead, they assume an IAM role that has the required permissions.
This provides temporary credentials to the service for secure operations.

Type of Policy	               Attached To                                 	Use Case
Resource-Based Policy	S3 Bucket, SNS, SQS, etc.	Directly grant access to specific resources (e.g., make a bucket public).

Identity-Based Policy	IAM Users, Groups, or Roles	Grant permissions to identities to access various AWS services/resources.

IAM Role with Policies	Services (e.g., Lambda, EC2)	Provide temporary credentials for AWS services to interact with resources.

Resource-Based Policies: Standalone policies directly attached to resources.
Identity-Based Policies: Require users, groups, or roles for attachment.
IAM Roles: Required for AWS services to assume permissions using policies.

what is  Action    = "sts:AssumeRole"  ?

The sts:AssumeRole action is part of AWS Security Token Service (STS) and is used to allow a trusted entity 
(e.g., a user, application, or service) to assume an IAM role. When an entity assumes a role, it temporarily gains the permissions defined by that role's attached policies.


Key Concepts
AWS Security Token Service (STS):

AWS STS provides temporary security credentials for IAM roles. These credentials are used to access AWS resources.
Assuming a Role:

A user, service, or another AWS account can assume a role to temporarily acquire its permissions.
The sts:AssumeRole action is required to allow an entity to assume the role.


Use Case for sts:AssumeRole
The sts:AssumeRole action is specified in the trust policy of an IAM role. It defines who can assume the role.

Example: Allowing EC2 to Assume a Role
You want an EC2 instance to access an S3 bucket. To do this, you:

Create an IAM role with s3 permissions.
Allow the EC2 service (ec2.amazonaws.com) to assume this role using sts:AssumeRole.
Trust Policy Example: This policy allows the EC2 service to assume the role:

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}

Structure of a Trust Policy
A trust policy uses sts:AssumeRole to define:

Effect: Allow or deny the assume role action.
Principal: Who can assume the role (e.g., services like EC2 or specific AWS accounts).
Action: The sts:AssumeRole action.

How sts:AssumeRole Works
Trust Policy:

Defines who (a user, service, or account) is allowed to assume the role.
AWS STS Call:

The trusted entity (e.g., EC2) makes an AssumeRole API call.
AWS STS generates temporary credentials for the role.
Temporary Credentials:

The temporary credentials include an access key, secret key, and session token.
These credentials allow the entity to access resources based on the permissions attached to the role.

Use Cases for sts:AssumeRole
Cross-Account Access:

Allow a user or role in Account B to access resources in Account A.
AWS Service Access:

Allow services like EC2, Lambda, or ECS tasks to assume roles for permissions.
Federated Access:

Allow external identity providers (e.g., Active Directory, Okta) to assume roles for accessing AWS resources.
Temporary Access:

Grant temporary access to a resource without needing to manage long-term credentials.


How It Looks in Terraform
In Terraform, the sts:AssumeRole action is typically used in the trust policy of a role.

Example:
Allow Lambda to assume a role:

resource "aws_iam_role" "lambda_role" {
  name = "lambda-access-s3-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect    = "Allow"
        Principal = { Service = "lambda.amazonaws.com" }
        Action    = "sts:AssumeRole"
      }
    ]
  })
}

Attach a policy to grant S3 access:

resource "aws_iam_role_policy" "lambda_policy" {
  name   = "lambda-s3-policy"
  role   = aws_iam_role.lambda_role.id
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect   = "Allow"
        Action   = ["s3:PutObject", "s3:GetObject"]
        Resource = "arn:aws:s3:::my-bucket/*"
      }
    ]
  })
}

sts:AssumeRole is an AWS STS action that allows a trusted entity (user, service, account) to temporarily assume an IAM role.
It’s defined in the trust policy of an IAM role.
Used for enabling cross-account access, AWS service permissions, and temporary credentials.

When creating a bucket for Terraform state storage (.tfstate), it should be private for security reasons. Terraform state files can contain sensitive information such as:

Access keys
Resource configurations
Secrets (e.g., database passwords, API keys)

Why the Bucket Should Be Private
Security: A public bucket exposes your infrastructure's sensitive data to anyone with access to the bucket URL.
Controlled Access: Only authorized users or roles should have access to the bucket.
No Need for CloudFront: CloudFront is not required for accessing the .tfstate bucket because:
Terraform clients directly access the bucket.
There’s no need for caching or CDN benefits for .tfstate files.


            'headers': {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'GET,OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type,Authorization'
            },

            mandatory :P 



# SUUUUUUUUPER IMPORTANT TO ADD ALL DEPENDS ON INTEGRATIONS!!!!!!!!! (WE HAD BIG PROBLEMS WITH THE API GATEWAY NOT BEING ABLE TO create a method GET/POST in THE CRYPTO LAMBDA)
# API Deployment (remove 'stage_name' attribute)
resource "aws_api_gateway_deployment" "api_deployment" {
  depends_on = [
    aws_api_gateway_integration.lambda_integration,
    aws_api_gateway_integration.send_cv_integration,
    aws_api_gateway_method.viewer_count_options,
    aws_api_gateway_method.send_cv_options,
    aws_api_gateway_integration.crypto_api_integration 
  ]
  rest_api_id = aws_api_gateway_rest_api.viewer_count_api.id
}

ALSO THE CODE worked within lambda but not if we tested inapi gateway! NEEDED A CODE REPAIR TOO! 

Request module was required for import requests in ctypto lambda

Double Quotes: If you're embedding JSON inside a string, double quotes need escaping 
because JSON strings are enclosed in double quotes.

{
  "body": "{\"email\": \"sam.albershtein@gmail.com\"}"
}

In SES Lambda and overall (we see it here because we're doing post)
That's how API Gatway send stuff 

When doing the test in API gatway we can type {"email": "sam.albershtein@gmail.com"} 
but the api gateway transforms it into {\"email\": \"sam.albershtein@gmail.com\"} (embeds into a string)
and send it in a body


You're absolutely right to ask this! When using signed URLs for CloudFront, Origin Access Control (OAC) or the older Origin Access Identity (OAI) can still be used, but with specific considerations:

Why OAC and Signed URLs Can Work Together
OAC secures the connection between CloudFront and S3, ensuring that S3 objects are only accessible through CloudFront.
Signed URLs secure access from the user to CloudFront, ensuring that only authorized users can access your content via CloudFront.
These mechanisms address different parts of the access flow and can coexist.

How to Configure Both Together
1. Origin Access Control (OAC)
When using OAC, you allow CloudFront to access the S3 bucket securely:

Use aws_cloudfront_origin_access_control to configure OAC in Terraform.
Update your S3 bucket policy to grant access to the OAC.
Note: This ensures that S3 only allows requests originating from CloudFront.

resource "aws_s3_bucket_policy" "cv_bucket_policy" {
  bucket = aws_s3_bucket.cv_bucket.id

  policy = jsonencode({
    Version = "2012-10-17",
    Statement = [
      {
        Sid       = "AllowCloudFrontAccess",
        Effect    = "Allow",
        Principal = {
          AWS = aws_cloudfront_origin_access_control.oac.arn
        },
        Action    = "s3:GetObject",
        Resource  = "arn:aws:s3:::${aws_s3_bucket.cv_bucket.id}/*"
      }
    ]
  })
}


2. Signed URLs
Signed URLs add another layer of security for access from users to CloudFront:

The trusted_signers block in CloudFront links your distribution to the CloudFront key-pair used to generate signed URLs.
This ensures that only users with valid signed URLs can access the cv.pdf object.

ordered_cache_behavior {
  path_pattern           = "cv.pdf"
  allowed_methods        = ["GET", "HEAD"]
  target_origin_id       = "S3-${aws_s3_bucket.cv_bucket.id}"
  viewer_protocol_policy = "redirect-to-https"

  forwarded_values {
    query_string = true
    cookies {
      forward = "none"
    }
  }

  trusted_signers = ["self"]
}

Final Note: Signed URLs + OAC Compatibility
Yes, you can use both OAC and signed URLs together, but:

Ensure that the S3 bucket policy allows CloudFront access with OAC.
Make sure the trusted_signers configuration is properly set in CloudFront.
Verify that the signed URL uses the correct CloudFront key-pair and includes all required parameters (Policy, Signature, and Key-Pair-Id).

1. Is CORS Mandatory for the S3 Bucket?
CORS (Cross-Origin Resource Sharing) is not mandatory for this scenario unless your web application or Lambda function is directly accessing the S3 bucket through the browser or client-side JavaScript.

In your case:

CORS is not required because access to the S3 bucket happens via CloudFront. CloudFront handles requests from clients and proxies them to S3, so the browser never directly communicates with S3.
CORS settings apply only to direct browser-to-S3 communication.
If your signed URLs work and you don’t plan to allow direct access to the bucket, you can safely remove the CORS configuration.

If you remove the CORS configuration, make sure there are no direct browser-to-S3 requests (which there shouldn't be if CloudFront is handling everything).



====

ADDITION TO WHAT WE DISCUSSED IN CLOUDFRONT ORIGIN OAC AND THE CV BUCKET (WE'VE CONFIGURED EVERYTHING BUT DIDN'T WANT OAC BECAUSE WE THOUGHT (SIGNED URL WILL NOT WORK WITH OAC BUT IN FACT THEY DEAL WITH DIFFERENT ENDS))
A VERY VERY TOUGH CHAIN OF TRUST TO GET MY CV, ONLY WITH A SPECIAL SIGNED URL TO ACCESS CLOUDFRONT->CV BUCKET OBJECT AND OAC WHICH LET'S CLOUDFRONT ACCESS THE BUCKET ITSELF

so origin_access_control_id = aws_cloudfront_origin_access_control.oac.id is a must when accessing buckets from cloudfront? 

Yes, origin_access_control_id = aws_cloudfront_origin_access_control.oac.id is mandatory when accessing private S3 buckets via CloudFront using the Origin Access Control (OAC) mechanism. Here's why:

Why Is origin_access_control_id Mandatory?
Access Control via AWS SigV4:

The origin_access_control_id links the CloudFront distribution to a specific OAC configuration.
OAC ensures that all requests from CloudFront to S3 are signed with AWS Signature Version 4 (SigV4). This is how CloudFront authenticates itself when accessing private S3 objects.
Replacing Legacy OAI (Origin Access Identity):

OAC is the modern replacement for the older Origin Access Identity (OAI) mechanism.
Without origin_access_control_id, CloudFront cannot apply SigV4 signing, and requests to S3 will be denied if the bucket is private.
S3 Bucket Security:

A private S3 bucket requires every request to be authenticated.
origin_access_control_id ensures that CloudFront can act on behalf of your distribution to retrieve objects from the bucket securely.
Integration with Bucket Policies:

The Principal in the S3 bucket policy (e.g., cloudfront.amazonaws.com) and the Condition with AWS:SourceArn depend on CloudFront making authenticated requests.
Without origin_access_control_id, CloudFront cannot make the necessary authenticated requests, resulting in an "Access Denied" error.
What Happens Without origin_access_control_id?
Requests Are Not Signed:

CloudFront will not sign requests to S3 with SigV4.
Since the S3 bucket policy allows only authenticated access, any unsigned requests will be denied.
Access Denied:

You will see errors like Access Denied when trying to retrieve objects via CloudFront because S3 rejects unauthenticated requests.
When Is origin_access_control_id Not Needed?
Public Buckets:

If your S3 bucket is public and does not require authentication, you can use CloudFront without an OAC. However, this is not recommended for secure setups.
Using S3 Signed URLs Instead of CloudFront:

If you are generating S3 Signed URLs (not CloudFront Signed URLs), then CloudFront does not interact directly with S3, and origin_access_control_id is not needed.
Best Practice
For private buckets accessed via CloudFront:

Use origin_access_control_id with an appropriate OAC configuration.
Ensure your S3 bucket policy is configured to allow access only to authenticated requests from CloudFront.
By using OAC and SigV4, you can maintain a secure and controlled integration between your CloudFront distribution and private S3 buckets.

CORS problem when pressing the button receive my cv : The 500 Internal Server Error for the OPTIONS request still indicates that the API Gateway is not properly handling CORS preflight 
requests when using the AWS_PROXY integration. The issue is that the Lambda function is expected to respond to the OPTIONS request explicitly with the correct CORS headers.

Here’s how to fix this once: Add an OPTIONS preflight in LAMBDA (IT;s AWSPROXY SO WE NEED TO DO IT MANUALLY LIKE WITH GET AND POST )

The CORS error you're encountering stems from the API Gateway not handling the preflight OPTIONS request properly. When your browser sends a POST request with custom headers like "Content-Type: application/json", it first sends an 
OPTIONS preflight request to verify if the server permits such requests from the origin. Since the API Gateway isn't configured to handle OPTIONS requests for the /send-cv resource, it results in a CORS error.


When using AWS_PROXY integration and your Lambda function is configured to handle CORS 
(including handling OPTIONS requests and returning the appropriate CORS headers), you do not need to set up MOCK integrations for the OPTIONS methods. The MOCK integrations become unnecessary in this scenario.

Key Takeaways
Separate Integrations for Each Method:

Even though you had an integration for POST, a separate OPTIONS method and its corresponding integration were necessary because the preflight request has its own lifecycle in the API Gateway.
Lambda Handles Preflight:

The AWS_PROXY type ensures that the Lambda function handles both POST and OPTIONS requests, simplifying CORS handling.
Test Thoroughly:

Always test the API behavior using tools like curl for both POST and OPTIONS requests, as you did.